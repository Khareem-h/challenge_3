{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  A Whale off the Port(folio)\n",
    " ---\n",
    "\n",
    " In this assignment, you'll get to use what you've learned this week to evaluate the performance among various algorithmic, hedge, and mutual fund portfolios and compare them against the S&P 500 Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "In this section, you will need to read the CSV files into DataFrames and perform any necessary data cleaning steps. After cleaning, combine all DataFrames into a single DataFrame.\n",
    "\n",
    "Files:\n",
    "\n",
    "* `whale_returns.csv`: Contains returns of some famous \"whale\" investors' portfolios.\n",
    "\n",
    "* `algo_returns.csv`: Contains returns from the in-house trading algorithms from Harold's company.\n",
    "\n",
    "* `sp500_history.csv`: Contains historical closing prices of the S&P 500 Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whale Returns\n",
    "\n",
    "Read the Whale Portfolio daily returns and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading whale returns\n",
    "#Set index by using: csv_df = pd.read_csv(xxx_csv, index_col=\"Date\", parse_dates=True)\n",
    "#Sort DataFrame using: .sort_index()\n",
    "#Remove nulls using: .dropna().copy() OR .dropna()\n",
    "\n",
    "whale_csv = Path(\"Resources/whale_returns.csv\")\n",
    "whale_df = pd.read_csv(whale_csv, index_col=\"Date\", parse_dates=True)\n",
    "official_whale_df = whale_df.sort_index().dropna()\n",
    "official_whale_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify nulls removed\n",
    "official_whale_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmic Daily Returns\n",
    "\n",
    "Read the algorithmic daily returns and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading algorithmic returns\n",
    "#Set index by using: csv_df = pd.read_csv(xxx_csv, index_col=\"Date\", parse_dates=True)\n",
    "#Sort DataFrame using: .sort_index()\n",
    "#Remove nulls using: .dropna().copy() OR .dropna()\n",
    "\n",
    "algo_csv = Path(\"Resources/algo_returns.csv\")\n",
    "algo_df = pd.read_csv(algo_csv, index_col=\"Date\", parse_dates=True)\n",
    "official_algo_df = algo_df.sort_index().dropna()\n",
    "official_algo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify nulls removed\n",
    "official_algo_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns\n",
    "official_algo_df = official_algo_df.rename(columns={\"Algo 1\":\"Algo 1 Returns\"})\n",
    "official_algo_df = official_algo_df.rename(columns={\"Algo 2\":\"Algo 2 Returns\"})\n",
    "official_algo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 Returns\n",
    "\n",
    "Read the S&P 500 historic closing prices and create a new daily returns DataFrame from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading S&P 500 Closing Prices\n",
    "#Reading algorithmic returns\n",
    "#Set index by using: csv_df = pd.read_csv(xxx_csv, index_col=\"Date\", parse_dates=True)\n",
    "#Sort DataFrame using: .sort_index()\n",
    "#Remove nulls using: .dropna().copy() OR .dropna()\n",
    "\n",
    "sp_csv = Path(\"Resources/sp500_history.csv\")\n",
    "sp_df = pd.read_csv(sp_csv, index_col=\"Date\", parse_dates=True)\n",
    "official_sp_df = sp_df.sort_index().dropna()\n",
    "official_sp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Data Types\n",
    "official_sp_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix data types\n",
    "official_sp_df[\"Close\"] = sp_df[\"Close\"].str.replace(\"$\", \"\")\n",
    "\n",
    "#Change daily returns to .astype(\"float\")\n",
    "official_sp_df[\"Close\"] = official_sp_df[\"Close\"].astype(\"float\")\n",
    "official_sp_df = official_sp_df.dropna()\n",
    "official_sp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Add daily returns column\n",
    "#Remove remaining nulls from data\n",
    "official_sp_df[\"Daily Returns\"] = official_sp_df[\"Close\"].pct_change()\n",
    "official_sp_df = official_sp_df.dropna()\n",
    "official_sp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename `Close` Column to be specific to this port\n",
    "official_sp_df = official_sp_df.rename(columns={\"Close\": \"S&P 500 Close\"})\n",
    "official_sp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Whale, Algorithmic, and S&P 500 Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Whale Returns, Algorithmic Returns, and the S&P 500 Returns into a single DataFrame with columns for each portfolio's returns.\n",
    "\n",
    "concat_df = pd.concat([official_sp_df, official_algo_df, official_whale_df], axis='columns', join='inner')\n",
    "\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Quantitative Analysis\n",
    "\n",
    "In this section, you will calculate and visualize performance and risk metrics for the portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "#### Calculate and Plot the daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import and clean Google Data\n",
    "#Set index by using: csv_df = pd.read_csv(xxx_csv, index_col=\"Date\", parse_dates=True)\n",
    "#Sort DataFrame using: .sort_index()\n",
    "#Remove nulls using: .dropna().copy() OR .dropna()\n",
    "goog_csv = Path(\"Resources/goog_historical.csv\")\n",
    "goog_df = pd.read_csv(goog_csv, index_col=\"Trade DATE\", parse_dates=True)\n",
    "good_df = goog_df.sort_index().dropna()\n",
    "good_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Rename NOCP column and convert to daily returns\n",
    "official_goog_df = goog_df.rename(columns={\"NOCP\":\"GOOG Daily Returns\"})\n",
    "official_goog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "official_goog_df[\"GOOG Daily Returns\"] = official_goog_df[\"GOOG Daily Returns\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "official_goog_df[\"GOOG Daily Returns\"] = official_goog_df[\"GOOG Daily Returns\"].pct_change()\n",
    "\n",
    "#Ensure nulls are removed and sorting adheres to \"ascending order\"\n",
    "official_goog_df = official_goog_df.dropna()\n",
    "official_goog_df = official_goog_df.sort_index()\n",
    "official_goog_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set index by using: csv_df = pd.read_csv(xxx_csv, index_col=\"Date\", parse_dates=True)\n",
    "#Sort DataFrame using: .sort_index()\n",
    "#Remove nulls using: .dropna().copy() OR .dropna()\n",
    "cost_csv = Path(\"Resources/cost_historical.csv\")\n",
    "cost_df = pd.read_csv(cost_csv, index_col=\"Trade DATE\", parse_dates=True)\n",
    "cost_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Clean cost_df...\n",
    "#-Remove nulls\n",
    "#-Sort data\n",
    "#-Convert to daily returns\n",
    "#-Rename columns(as needed)\n",
    "sorted_cost_df = cost_df.sort_index()\n",
    "official_cost_df = sorted_cost_df.rename(columns={\"NOCP\":\"COST Daily Returns\"})\n",
    "official_cost_df[\"COST Daily Returns\"] = official_cost_df[\"COST Daily Returns\"].astype(float)\n",
    "official_cost_df[\"COST Daily Returns\"] = official_cost_df[\"COST Daily Returns\"].pct_change()\n",
    "official_cost_df = official_cost_df.dropna()\n",
    "official_cost_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Clean and prepare APPLE Stock data:\n",
    "#Apple daily returns, sorted data, remove NAs\n",
    "#Set index by using: csv_df = pd.read_csv(xxx_csv, index_col=\"Date\", parse_dates=True)\n",
    "#Sort DataFrame using: .sort_index()\n",
    "#Remove nulls using: .dropna().copy() OR .dropna()\n",
    "aapl_csv = Path(\"Resources/aapl_historical.csv\")\n",
    "aapl_df = pd.read_csv(aapl_csv,index_col=\"Trade DATE\",parse_dates=True)\n",
    "aapl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Clean aaplt_df...\n",
    "#-Remove nulls\n",
    "#-Sort data\n",
    "#-Convert to daily returns\n",
    "#-Rename columns(as needed)\n",
    "sorted_aapl_df = aapl_df.sort_index()\n",
    "sorted_aapl_df[\"NOCP\"] = sorted_aapl_df[\"NOCP\"].astype(float)\n",
    "sorted_aapl_df[\"NOCP\"] = sorted_aapl_df[\"NOCP\"].pct_change()\n",
    "sorted_aapl_df = sorted_aapl_df.dropna()\n",
    "sorted_aapl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "official_aapl_df = sorted_aapl_df.rename(columns={\"NOCP\":\"AAPL Daily Returns\"})\n",
    "official_aapl_df = official_aapl_df.dropna()\n",
    "official_aapl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL DATA HAS BEEN CLEANED..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create and refine concatenation for...\n",
    "#Daily Returns // Total Dataframes // Cumulative Returns\n",
    "concat_df = pd.concat([official_sp_df, official_algo_df, official_whale_df], axis='columns', join='inner')\n",
    "daily_returns_concat_df = pd.concat([official_sp_df[\"Daily Returns\"],official_algo_df[\"Algo 1 Returns\"],official_algo_df[\"Algo 2 Returns\"],official_whale_df], axis=\"columns\",join=\"inner\")                                                        \n",
    "total_concat = pd.concat([concat_df,official_goog_df, official_cost_df, official_aapl_df], axis=\"columns\",join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total__daily_returns_concat = pd.concat([daily_returns_concat_df,official_goog_df[\"GOOG Daily Returns\"],official_cost_df[\"COST Daily Returns\"],official_aapl_df[\"AAPL Daily Returns\"]], axis=\"columns\",join=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate and Plot cumulative returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative returns of all portfolios\n",
    "cumulative_returns = (1 + total__daily_returns_concat).cumprod() - 1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot cumulative returns\n",
    "cumulative_returns.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot daily returns of all portfolios\n",
    "total__daily_returns_concat.plot(title=\"Daily Returns (All Portfolios)\",figsize=(20,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Analysis\n",
    "\n",
    "Determine the _risk_ of each portfolio:\n",
    "\n",
    "1. Create a box plot for each portfolio. \n",
    "2. Calculate the standard deviation for all portfolios\n",
    "4. Determine which portfolios are riskier than the S&P 500\n",
    "5. Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a box plot for each portfolio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot to visually show risk\n",
    "total__daily_returns_concat.plot(kind='box',figsize=(30,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Standard Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the daily standard deviations of all portfolios\n",
    "daily_std = total__daily_returns_concat.std()*100\n",
    "daily_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which portfolios are riskier than the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate  the daily standard deviation of S&P 500\n",
    "sp500_std = official_sp_df[\"Daily Returns\"].std()*100\n",
    "print(f\"The S&P 500 standard deviation for Daily Returns is {sp500_std}\")\n",
    "\n",
    "# Determine which portfolios are riskier than the S&P 500\n",
    "total__daily_returns_concat.std()>total__daily_returns_concat[\"Daily Returns\"].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Display total daily returns std\n",
    "total__daily_returns_concat[\"Daily Returns\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the annualized standard deviation (252 trading days)\n",
    "annual_std = total__daily_returns_concat.std()*np.sqrt(252)\n",
    "annual_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Statistics\n",
    "\n",
    "Risk changes over time. Analyze the rolling statistics for Risk and Beta. \n",
    "\n",
    "1. Calculate and plot the rolling standard deviation for all portfolios using a 21-day window\n",
    "2. Calculate the correlation between each stock to determine which portfolios may mimick the S&P 500\n",
    "3. Choose one portfolio, then calculate and plot the 60-day rolling beta between it and the S&P 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot rolling `std` for all portfolios with 21-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rolling standard deviation for all portfolios using a 21-day window\n",
    "# Plot the rolling standard deviation\n",
    "total__daily_returns_concat.rolling(window=21).std().plot(figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation\n",
    "total_returns_corr = total__daily_returns_concat.corr()\n",
    "# Display de correlation matrix\n",
    "total_returns_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and Plot Beta for a chosen portfolio and the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display de correlation matrix via heatmap\n",
    "sns.heatmap(total_returns_corr, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total__daily_returns_concat[\"Algo 1 Returns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total__daily_returns_concat[\"Daily Returns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance of a single portfolio\n",
    "algo_df_cov = total__daily_returns_concat[\"Algo 1 Returns\"].rolling(window=10).cov(total__daily_returns_concat[\"Daily Returns\"])\n",
    "algo_df_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate variance of S&P 500\n",
    "sp_df_var = total__daily_returns_concat[\"Daily Returns\"].rolling(window=10).var()\n",
    "sp_df_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Computing beta\n",
    "compared_beta = algo_df_cov/sp_df_var\n",
    "compared_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot beta trend\n",
    "compared_beta.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Statistics Challenge: Exponentially Weighted Average \n",
    "\n",
    "An alternative way to calculate a rolling window is to take the exponentially weighted moving average. This is like a moving window average, but it assigns greater importance to more recent observations. Try calculating the [`ewm`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html) with a 21-day half life for each portfolio, using standard deviation (`std`) as the metric of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `ewm` to calculate the rolling window\n",
    "total__daily_returns_concat.ewm(halflife=21).std().plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpe Ratios\n",
    "In reality, investment managers and thier institutional investors look at the ratio of return-to-risk, and not just returns alone. After all, if you could invest in one of two portfolios, and each offered the same 10% return, yet one offered lower risk, you'd take that one, right?\n",
    "\n",
    "### Using the daily returns, calculate and visualize the Sharpe ratios using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualized Sharpe Ratios\n",
    "sharpe_concat = (total__daily_returns_concat.mean()*252)/(total__daily_returns_concat.std()*np.sqrt(252))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sharpe ratios as a bar plot\n",
    "sharpe_concat.plot(kind=\"bar\",title=\"Portfolio Sharpe Ratios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine whether the algorithmic strategies outperform both the market (S&P 500) and the whales portfolios.\n",
    "\n",
    "Write your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The first algorithm (Algo 1) outperforms the market regarding risk vs return. The second\")\n",
    "print(f\"algorithm (Algo 2) performs remarkably poorly against the S&P 500!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create Custom Portfolio\n",
    "\n",
    "In this section, you will build your own portfolio of stocks, calculate the returns, and compare the results to the Whale Portfolios and the S&P 500. \n",
    "\n",
    "1. Choose 3-5 custom stocks with at last 1 year's worth of historic prices and create a DataFrame of the closing prices and dates for each stock.\n",
    "2. Calculate the weighted returns for the portfolio assuming an equal number of shares for each stock\n",
    "3. Join your portfolio returns to the DataFrame that contains all of the portfolio returns\n",
    "4. Re-run the performance and risk analysis with your portfolio to see how it compares to the others\n",
    "5. Include correlation analysis to determine which stocks (if any) are correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose 3-5 custom stocks with at last 1 year's worth of historic prices and create a DataFrame of the closing prices and dates for each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 1st stock\n",
    "#Set index by using: csv_df = pd.read_csv(xxx_csv, index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n",
    "intc_path = Path(\"Resources/intc_new.csv\")\n",
    "intc_df = pd.read_csv(intc_path, index_col=\"Date\", parse_dates=True)\n",
    "intc_df.index = intc_df.index.date\n",
    "intc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Rename columns for clarity and create a daily returns column\n",
    "official_intc_df = intc_df.rename(columns={\"Close\":\"INTC Close\"})\n",
    "official_intc_df[\"INTC Daily Returns\"] = official_intc_df[\"INTC Close\"]\n",
    "official_intc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Change data to float and generate daily returns\n",
    "official_intc_df = official_intc_df.astype(float)\n",
    "official_intc_df[\"INTC Daily Returns\"]=official_intc_df[\"INTC Daily Returns\"].pct_change()\n",
    "official_intc_df = official_intc_df.dropna()\n",
    "official_intc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 2nd stock\n",
    "#Set index by using: csv_df = pd.read_csv(xxx_csv, index_col=\"Date\", parse_dates=True)\n",
    "jpm_path = Path(\"Resources/jpm_new.csv\")\n",
    "jpm_df = pd.read_csv(jpm_path, index_col=\"Date\", parse_dates=True)\n",
    "jpm_df.index = jpm_df.index.date\n",
    "jpm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Rename columns for clarity and create a daily returns column\n",
    "official_jpm_df = jpm_df.rename(columns={\"Close\":\"JPM Close\"})\n",
    "official_jpm_df[\"JPM Daily Returns\"] = official_jpm_df[\"JPM Close\"]\n",
    "official_jpm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Change data to float and generate daily returns\n",
    "official_jpm_df = official_jpm_df.astype(float)\n",
    "official_jpm_df[\"JPM Daily Returns\"]=official_jpm_df[\"JPM Daily Returns\"].pct_change()\n",
    "official_jpm_df = official_jpm_df.dropna()\n",
    "official_jpm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 3rd stock\n",
    "#Set index by using: csv_df = pd.read_csv(xxx_csv, index_col=\"Date\", parse_dates=True)\n",
    "axp_path = Path(\"Resources/axp_new.csv\")\n",
    "axp_df = pd.read_csv(axp_path, index_col=\"Date\", parse_dates=True)\n",
    "axp_df.index=axp_df.index.date\n",
    "axp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Rename columns for clarity and create a daily returns column\n",
    "official_axp_df = axp_df.rename(columns={\"Close\":\"AXP Close\"})\n",
    "official_axp_df[\"AXP Daily Returns\"] = official_axp_df[\"AXP Close\"]\n",
    "official_axp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Change data to float and generate daily returns\n",
    "official_axp_df = official_axp_df.astype(float)\n",
    "official_axp_df[\"AXP Daily Returns\"]=official_axp_df[\"AXP Daily Returns\"].pct_change()\n",
    "official_axp_df = official_axp_df.dropna()\n",
    "official_axp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all stocks in a single DataFrame\n",
    "concat_stocks = pd.concat([official_axp_df[\"AXP Daily Returns\"],official_jpm_df[\"JPM Daily Returns\"],official_intc_df[\"INTC Daily Returns\"]],axis=\"columns\", join=\"inner\")\n",
    "concat_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Rest Date Index\n",
    "concat_stocks_sorted = concat_stocks.sort_index()\n",
    "concat_stocks_sorted\n",
    "concat_stocks_sorted.index=concat_stocks_sorted.index.date\n",
    "concat_stocks_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the weighted returns for the portfolio assuming an equal number of shares for each stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights\n",
    "weights = [1/3, 1/3, 1/3]\n",
    "# Calculate portfolio return\n",
    "portfolio_returns = concat_stocks_sorted.dot(weights)\n",
    "# Display sample data\n",
    "portfolio_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "portfolio_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(portfolio_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total__daily_returns_concat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "portfolio_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join your portfolio returns to the DataFrame that contains all of the portfolio returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total__daily_returns_concat[\"Custom Portfolio\"] = portfolio_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only compare dates where return data exists for all the stocks (drop NaNs)\n",
    "total__daily_returns_concat = total__daily_returns_concat.dropna()\n",
    "total__daily_returns_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run the risk analysis with your portfolio to see how it compares to the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the annualized `std`\n",
    "ann_concat_std = total__daily_returns_concat.std()*np.sqrt(252)\n",
    "ann_concat_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot rolling `std` with 21-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling standard deviation\n",
    "# Plot rolling standard deviation\n",
    "total__daily_returns_concat.rolling(window=21).std().plot(figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the correlation\n",
    "total_returns_corr2 = total__daily_returns_concat.corr()\n",
    "total_returns_corr2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and Plot Rolling 60-day Beta for Your Portfolio compared to the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot Beta\n",
    "custom_cov = total__daily_returns_concat[\"Custom Portfolio\"].rolling(window=60).cov(total__daily_returns_concat[\"Daily Returns\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_beta = custom_cov/sp_df_var\n",
    "custom_beta.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the daily returns, calculate and visualize the Sharpe ratios using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Annualized Sharpe Ratios\n",
    "total_sharpe = (total__daily_returns_concat.mean()*252)/(total__daily_returns_concat.std()*np.sqrt(252))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sharpe ratios as a bar plot\n",
    "total_sharpe.plot(kind='bar',figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does your portfolio do?\n",
    "\n",
    "Write your answer here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"My portfolio is the 4th strongest amongst the selection. It is the fourth best option to choose regarding\")\n",
    "print(f\"risk vs return.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
